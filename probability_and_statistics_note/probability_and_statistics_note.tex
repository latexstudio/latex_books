% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\documentclass[11pt]{book}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{ctex}

\usepackage{tikz}
\usepackage{xcolor}

\usepackage{environ}
\usepackage{amsmath,mathrsfs,amsfonts}
\usepackage{xparse}

\usepackage{pgfplots}
\usepackage[colorlinks, CJKbookmarks]{hyperref}

\newtheorem{exercise}{\hspace{2em}\textbf{例}}[section]
\newtheorem{definition}{\hspace{2em}\textbf{定义}}


\usetikzlibrary{shapes,decorations}
\definecolor{bule}{RGB}{18,29,57}
\definecolor{bablue}{RGB}{248,248,248}
\definecolor{main}{RGB}{127,191,51}
\definecolor{seco}{RGB}{0,145,215}
\definecolor{thid}{RGB}{180,27,131}


\newcommand{\newfancytheoremstyle}[5]{%
  \tikzset{#1/.style={draw=#3, fill=#2,very thick,rectangle,
      rounded corners, inner sep=10pt, inner ysep=20pt}}
  \tikzset{#1title/.style={fill=#3, text=#2}}
  \expandafter\def\csname #1headstyle\endcsname{#4}
  \expandafter\def\csname #1bodystyle\endcsname{#5}
}

\newfancytheoremstyle{fancythrm}{blue!10}{seco}{\bfseries\sffamily}{\sffamily}

\makeatletter
\DeclareDocumentCommand{\newfancytheorem}{ O{\@empty} m m m O{fancythrm} }{%
  % define the counter for the theorem
  \ifx#1\@empty
    \newcounter{#2}
  \else
    \newcounter{#2}[#1]
    \numberwithin{#2}{#1}
  \fi
  %% define the "newthem" environment
  \NewEnviron{#2}[1][{}]{%
    \noindent\centering
    \begin{tikzpicture}
      \node[#5] (box){
        \begin{minipage}{0.93\columnwidth}
          \csname #5bodystyle\endcsname \BODY~##1
        \end{minipage}};
      \node[#5title, right=10pt] at (box.north west){
        {\csname #5headstyle\endcsname #3 \stepcounter{#2}\csname the#2\endcsname\; ##1}};
      \node[#5title, rounded corners] at (box.east) {#4};
    \end{tikzpicture}
  }[\par\vspace{.5\baselineskip}]
}
\makeatother


% Define new styles
% \newfancytheoremstyle{<name>}{inner color}{outer color}{head style}{body style}
\newfancytheoremstyle{fancydef}{green!10}{green}{\itshape\sffamily}{\sffamily}

% Define some new environments
% \newfancytheorem[<number within>]{<name>}{<head>}{<symbol>}[<style>]
\newfancytheorem[chapter]{newthem}{Theorem}{$\clubsuit$}
\newfancytheorem[section]{newcor}{Corollary 推论}{$\heartsuit$}
\newfancytheorem{newdef}{定义}{$\spadesuit$}[fancydef]



\usepackage{makeidx} % 务必放到最后

\title{概率论与数理统计学习笔记}
\author{Roger Young}
\setCJKmainfont{楷体}

\date{编译时间：\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\chapter{\LaTeX 基本使用说明}
	\section{入门常识}
	
	\LaTeX 会默认丢弃命令后的空白字符。如果希望在命令后的到一个空白字符，可以在命令后加上\{\}和一个空格，或者加上一个特殊的空白距离命令。
	\LaTeX 源文件中可以使用\%来表示注释，\LaTeX 在处理文件时，会忽略\%后的改行剩余文本。如果需要使用较长的注释（注释块），可以使用verbatim包。
		\section{页面布局}
				\subsection{分页、分段、断行和断字}
		有必要时\LaTeX 会进行必要的分页、断行和断字。在特殊情况下，需要使用命令指示\LaTeX 进行分页、分段、断行和断字。这些命令包括：
		\begin{itemize}
			\item $ \backslash \backslash $ ：连着的两个反斜线指示\LaTeX 另起一行，但不另起一段。
			\item $ \backslash newline $ 命令：等同于$ \backslash \backslash $；
			\item $ \backslash \backslash* $ 命令：强行断行后，还禁止分页；
			\item $ \backslash newpage $ 命令：另起一新页。
			\item $ \backslash hyphenation\{word list\} $ ：可以用来断字；
			\item $ \backslash- $：提示断字位置。
			\item $ \backslash linebreak $：强制断行；
			\item $ \backslash nolinebreak $：强制不断行；
			\item $ \backslash linebreak[priority] $：建议断行，priority可选值为0、1、2、3、4；
			\item $ \backslash nolinebreak[priority] $：建议不断行，priority可选值为0、1、2、3、4；
			\item $ \backslash cleardoublepage $ ：开始新的偶数页；
			\item $ \backslash clearpage $：开始新页，并且导致当前浮动的表格、图片等都输出。；
			\item $ \backslash newpage $：开始新页；
			\item $ \backslash enlargethispage\left\lbrace size\right\rbrace  $：扩大当前页；
			\item $ \backslash pagebreak $：强制分页；
			\item $ \backslash nopagebreak $：强制不分页。
			
		\end{itemize}
		\subsection{页眉设置}
		\subsection{页脚设置}
		\begin{tabular}{|l|l|}
			\hline
			$ \backslash footnote[number]\left\lbrace text\right\rbrace $ & 在当前页尾插入编号的脚标 \\
			\hline
		\end{tabular}
		\section{字体设置}

		\section{参考文献引用}
		\section{\LaTeX 环境简介}
		\section{数学公式}
		\section{作图功能}
		\LaTeX 可以通过内置的picture环境来创建简单的图形。对于创建更复杂的图形可以尝试以下包：
		\begin{itemize}
			\item beamer
			\item pgf, Portable Graphics Format
			\item tikz, TikZ
		\end{itemize}
	
		一个picture环境可以通过“$ \backslash begin\left\{picture\right\}(x, y)$ ”和“$ \backslash end\left\{picture\right\}$ ”，或者“$ \backslash begin\left\{picture\right\}(x, y)(x_0, y_0)$ ”和“$ \backslash end\left\{picture\right\}$”来创建。其中$ x $、$ y $、$ x_0 $、$ y_0 $和$ \backslash unitlength $相关。$ (x, y) $ 指示\LaTeX 为picture预留的大小。可选的参数$ (x_0, y_0)$是为picture预留的方形的的左下角的坐标。
		可以通过“$ \backslash setlength$ ”命令来设置：
		\begin{center}
			$ \backslash setlength\left\lbrace \backslash unitlength \right\rbrace \left\lbrace 1.2cm \right\rbrace$
		\end{center}
	
		绘图命名的格式一般如下：
		\begin{center}
			$ \backslash put(x, y)\left\lbrace object\right\rbrace  $
		\end{center}
	或者
	\begin{center}
		$ \backslash multiput(x, y)(\Delta x, \Delta y)\left\lbrace n\right\rbrace\left\lbrace object\right\rbrace  $
	\end{center}
	贝塞尔曲线是个例外，其格式为：
		\begin{center}
		$ \backslash qbezier(x_1, y_1)(x_2, y_2)(x_3, y_3) $
	\end{center}


常用的画图命令如下：\\
\begin{center}
	\begin{tabular}{|c|l|c|}
		\hline
		类型 & 命令 & 说明 \\
		\hline
		线段 & $ \backslash put(x, y)\left \lbrace \backslash line(x_1, y_1)\left\lbrace length\right\rbrace \right\rbrace $ & \\
		\hline
		箭头 & $ \backslash put(x, y)\left \lbrace \backslash vector(x_1, y_1)\left\lbrace length\right \rbrace \right \rbrace $ & \\		
		\hline
		圆 & $ \backslash put(x, y)\left \lbrace \backslash circle\left \lbrace diameter\right \rbrace \right \rbrace  $ & \\
		\hline
		文本 & $ \backslash put(x, y)\left \lbrace \$\ text\ \$ \right\rbrace  $ & \\ 
		\hline
		公式 & $ \backslash put(x, y)\left \lbrace \$\ formulas\ \$ \right\rbrace  $ & \\ 
		\hline
		椭圆形 & $ \backslash put(x, y)\left \lbrace \backslash oval(x, y)\right \rbrace $ & \\ 
		\hline
		椭圆形 & $ \backslash put(x, y)\left \lbrace \backslash oval(x, y)[position]\right \rbrace $ & \\ 
		\hline
		贝叶斯曲线 & $ \backslash qbezier(x_1, y_1)(x_2, y_2)(x_3, y_3) $ & \\ 
		\hline
	\end{tabular}
\end{center}
\begin{tikzpicture}
\begin{axis} [axis lines=center]
\addplot [domain=-3:3, thick, smooth] { x^3 - 5*x };
\end{axis}
\end{tikzpicture}
\chapter{机器学习的数学基础}
\section{概述}
我们知道，机器学习的特点就是：以计算机为工具和平台，以数据为研究对象，以学习方法为中心；是概率论、线性代数、数值计算、信息论、最优化理论和计算机科学等多个领域的交叉学科。所以本文就先介绍一下机器学习涉及到的一些最常用的的数学知识。
\section{线性代数}
\subsection{标量}
一个标量就是一个单独的数，一般用小写的的变量名称表示。
\subsection{向量}
一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排
列成一个方括号包围的纵柱：

我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。

\subsection{矩阵}
矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如\textbf{A}。 如果一个实数矩阵高度为$ m $，宽度为$ n $，那么我们说$ A \in R^{m \times n} $。
\begin{center}
	\begin{equation}
  A= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n}\\ a_{21} & a_{22} & \cdots & a_{2n}\\ \cdots & \cdots & & \cdots\\ a_{m1} & a_{m2} & \cdots & a_{mn}\\ \end{bmatrix}
  \end{equation}
\end{center}
\subsection{张量}
几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格： 


其中表的横轴表示图片的宽度值，这里只截取0~319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）。

当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。

\subsection{范数}
有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。Lp 范数如下：
\begin{center}
	\begin{equation}
\left \| x \right \|_{p}\left ( \sum_{i}\left | x_{i} \right |^{p}  \right )^{\frac{1}{p}}
\end{equation}
\end{center}

所以：

L1范数$ \left| \left| x \right| \right| $：为x向量各个元素绝对值之和；

L2范数$ \left| \left| x \right|  \right| _{2} $ ：为x向量各个元素平方和的开方。

\subsection{特征分解}
许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量$ \nu $：

$ A\nu =\lambda \nu $

标量$ \lambda $被称为这个特征向量对应的特征值。 

使用特征分解去分析矩阵A时，得到特征向量构成的矩阵V和特征值构成的向量$ \lambda $，我们可以重新将A写作：

$ A=Vdiag\left( \lambda  \right) V^{-1} $

\subsection{奇异值分解（SVD）}
除了特征分解，还有一种分解矩阵的方法，被称为奇异值分解（SVD）。将矩阵分解为奇异向量和奇异值。通过奇异分解，我们会得到一些类似于特征分解的信息。然而，奇异分解有更广泛的应用。

每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。
奇异分解与特征分解类似，只不过这回我们将矩阵A分解成三个矩阵的乘积：

\begin{center}
	$ A=UDV^{T} $ 
\end{center}
假设A是一个$ m\times n $矩阵，那么U是一个$ m\times m $矩阵，D是一个$ m\times n $矩阵，V是一个$ n\times n $矩阵。

这些矩阵每一个都拥有特殊的结构，其中$ U $和$ V $都是正交矩阵，$ D $是对角矩阵（注意，$ D $不一定是方阵）。对角矩阵$ D $对角线上的元素被称为矩阵$ A $的奇异值。矩阵U的列向量被称为左奇异向量，矩阵V 的列向量被称右奇异向量。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。
\subsection{Moore-Penrose伪逆}

对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：

\begin{center}\begin{equation} Ax=y \end{equation}\end{center}
等式两边同时左乘左逆B后，得到：

\begin{center}\begin{equation}x=By\end{equation}\end{center}
是否存在唯一的映射将A映射到B取决于问题的形式。

如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。

Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为：

但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：

其中，矩阵$ U $，$ D $ 和$ V $ 是矩阵$ A $奇异值分解后得到的矩阵。对角矩阵$ D $ 的伪逆$ D^{+} $ 是其非零元素取倒之后再转置得到的。

\subsection{几种常用的距离}

设有两个n维变量$ A=\left[ x_{11}, x_{12},...,x_{1n}   \right] $和$ B=\left[ x_{21} ,x_{22} ,...,x_{2n}  \right] $ ，则下面可以定义一些常用的距离公式：

\paragraph{曼哈顿距离}

曼哈顿距离也称为城市街区距离，数学定义如下：

$ d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } $ 

曼哈顿距离的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sum(abs(vector1-vector2))
\paragraph{欧氏距离}

欧氏距离其实就是L2范数，数学定义如下： 

$ d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k}  \right) ^{2} } }  $
欧氏距离的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt((vector1-vector2)*(vector1-vector2).T)
\paragraph{闵可夫斯基距离}

从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：

$ d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k}  \right) ^{p} } }  $
实际上，当$ p=1 $时，就是曼哈顿距离；当$ p=2 $时，就是欧式距离。

\paragraph{切比雪夫距离}

切比雪夫距离就是$ L_{\varpi} $ ，即无穷范数，数学表达式如下：

$ d_{12} =max\left( \left| x_{1k}-x_{2k} \right|  \right)  $
切比雪夫距离额Python实现如下：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt(abs(vector1-vector2).max)
\paragraph{夹角余弦}

夹角余弦的取值范围为$ [-1,1] $，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。

机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：

$ cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } }  $
夹角余弦的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))
\paragraph{汉明距离}

汉明距离定义的是两个字符串中不相同位数的数目。

例如：字符串$ ‘1111’ $与$ ‘1001’ $之间的汉明距离为2。

信息编码中一般应使得编码间的汉明距离尽可能的小。

汉明距离的Python实现：

from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print smstr
\paragraph{杰卡德相似系数}

两个集合$ A $和$ B $的交集元素在$ A $和$  $B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号$ J(A,B) $表示，数学表达式为：

$ J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| }  $
杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。

\paragraph{杰卡德距离}

与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：

$ J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| }  $
杰卡德距离的Python实现：

from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print dist.pdist(matV,'jaccard')
\section{概率}

\subsection{为什么使用概率}？ 

概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。

在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。

计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。

\subsection{随机变量}

随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，$ x_{1} $ 和$ x_{2} $  都是随机变量$ X $可能的取值。
对于向量值变量，我们会将随机变量写成$ X $，它的一个值为$ x $。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。

随机变量可以是离散的或者连续的。

\subsection{概率分布}

给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。
从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。
\subsection{条件概率}

很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。
我们将给定$ X=x $时，$ Y=y $发生的概率记为$ P\left( Y=y|X=x \right)  $，这个概率可以通过下面的公式来计算：

$ P\left( Y=y|X=x  \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }  $

\subsection{贝叶斯公式}

先看看什么是“先验概率”和“后验概率”，以一个例子来说明：

假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有：$ P(患病) = 0.1\% $；即：在没有做检验之前，我们预计的患病率为$ P(患病)=0.1\% $，这个就叫作"先验概率"。 

再假设现在有一种该病的检测方法，其检测的准确率为$ 95\% $；即：如果真的得了这种病，该检测法有95\%的概率会检测出阳性，但也有5\%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有95\%的概率检测出阴性，但也有5\%的概率检测为阳性。用概率条件概率表示即为：$ P(显示阳性|患病)=95\% $

现在我们想知道的是：在做完检测显示为阳性后，某人的患病率$ P(患病|显示阳性) $，这个其实就称为"后验概率"。

而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”，如下：

$ P\left( B|A \right) =\frac{P\left( A|B \right)P\left( B\right)  }{P\left( A \right) }  $
在这个例子里就是：

贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。

这里需要说明的是，上面的计算中除了利用了贝叶斯公式外，还利用了“全概率公式”，即：

$ P\left( A \right) =P\left( A|B \right)+P\left( A|\bar{B}  \right)   $

\subsection{期望}

在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。

假设$  $X是一个离散随机变量，其可能的取值有：$ \left\{ x_{1} ,x_{2} ,......,x_{n}  \right\} $ ，各个取值对应的概率取值为：$ P\left( x_{k} \right)  , k=1,2,......,n $，则其数学期望被定义为：

$ E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k}  \right) }  $
假设X是一个连续型随机变量，其概率密度函数为$ P\left( x \right) $ 则其数学期望被定义为：

$ E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx $

\subsection{方差 }

概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： 

$ Var\left( x \right) =E\left\{ \left[ x-E\left( x \right)  \right] ^{2}   \right\} =E\left( x^{2}  \right) -\left[ E\left( x \right)  \right] ^{2}  $

\subsection{协方差}

在概率论和统计学中，协方差被用于衡量两个随机变量X和Y之间的总体误差。数学定义式为：

$ Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right]  \right) \left( Y-E\left[ Y \right]  \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right] $ 

\subsection{常见分布函数}

\paragraph{0-1分布}

0-1分布是单个二值型离散随机变量的分布，其概率分布函数为：

$ P\left( X=1 \right) =pP\left( X=0 \right) =1-p $
\paragraph{几何分布}

几何分布是离散型概率分布，其定义为：在$ n $次伯努利试验中，试验$ k $次才得到第一次成功的机率。即：前$ k-1 $次皆失败，第$ k $次成功的概率。其概率分布函数为：

$ P\left( X=k \right) =\left( 1-p \right) ^{k-1} p $
性质：

$ E\left( X \right) =\frac{1}{p} Var\left( X \right) =\frac{1-p}{p^{2} }  $
\paragraph{二项分布}

二项分布即重复$ n $次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为$ p $，不发生的概率为$ 1-p $，则$ n $次重复独立试验中发生$ k $次的概率为：

$ P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}  $
性质：

$ E\left( X \right) =npVar\left( X \right) =np\left( 1-p \right)  $

\paragraph{高斯分布}

高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：


若随机变量$ X $服从一个数学期望为$ \mu $ ，方差为$ \sigma ^{2} $ 的正态分布，则我们将其记为：$ N\left( \mu ,\sigma^{2}  \right)  $。其期望值$ \mu $ 决定了正态分布的位置，其标准差$ \sigma $ （方差的开方）决定了正态分布的幅度。

\paragraph{指数分布}

指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为$ T $，已知元件使用了$ t $小时，它总共使用至少$ t+s $小时的条件概率，与从开始使用时算起它使用至少$ s $小时的概率相等。下面这些都属于指数分布：

婴儿出生的时间间隔
网站访问的时间间隔
奶粉销售的时间间隔 
指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间$ t $，就等同于$ t $之内没有任何婴儿出生，即：

$ P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t}  }{0!}=e^{-\lambda t} $ 
则：

$ P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}  $
如：接下来15分钟，会有婴儿出生的概率为：

$ P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53 $
指数分布的图像如下：


\paragraph{泊松分布}

日常生活中，大量事件是有固定频率的，比如：

某医院平均每小时出生3个婴儿
某网站平均每分钟有2次访问
某超市平均每小时销售4包奶粉 
它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。

泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为：

$ P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t}  }{n!}  $
其中：

$ P $表示概率，$ N $表示某种函数关系，$ t $表示时间，$ n $表示数量，$ 1 $小时内出生3个婴儿的概率，就表示为$  P(N(1) = 3)  $；$ λ $ 表示事件的频率。

还是以上面医院平均每小时出生3个婴儿为例，则$ \lambda =3 $；

那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：

$ P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025 $
同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：

$ P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0  \right) - P\left( N\left( 1 \right)=1  \right)\approx 0.8 $
【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！

\subsection{Lagrange乘子法}

对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。

对于一个要求极值的函数$f\left( x,y \right) $，图上的蓝圈就是这个函数的等高图，就是说$ f\left( x,y \right) =c_{1} ,c_{2} ,...,c_{n} $分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组$\left( x,y \right)$ ，使它的c_{i} 值越大越好，但是这点必须满足约束条件$g\left( x,y \right)$（在黄线上）。

也就是说f(x,y)和g(x,y)相切，或者说它们的梯度▽f和▽g平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为\lambda 倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。

\subsection{最大似然法}

最大似然也称为最大概似估计，即：在“模型已定，参数$ θ $未知”的情况下，通过观测数据估计未知参数$ θ $ 的一种思想或方法。

其基本思想是： 给定样本取值后，该样本最有可能来自参数$ \theta $ 为何值的总体。即：寻找$ \tilde{\theta } _{ML} $ 使得观测到样本数据的可能性最大。

举个例子，假设我们要统计全国人口的身高，首先假设这个身高服从服从正态分布，但是该分布的均值与方差未知。由于没有足够的人力和物力去统计全国每个人的身高，但是可以通过采样（所有的采样要求都是独立同分布的），获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值与方差。

求极大似然函数估计值的一般步骤：

1、写出似然函数；

2、对似然函数取对数；
3、两边同时求导数；
4、令导数为$ 0 $解出似然方程。
最大似然估计也是统计学习中经验风险最小化的例子。如果模型为条件概率分布，损失函数定义为对数损失函数，经验风险最小化就等价于最大似然估计。


\section{信息论}

信息论是应用数学的一个分支，主要研究的是对一个信号能够提供信息的多少进行量化。如果说概率使我们能够做出不确定性的陈述以及在不确定性存在的情况下进行推理，那信息论就是使我们能够量化概率分布中不确定性的总量。

1948年，香农引入信息熵，将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。
\subsection{熵}

如果一个随机变量$ X $的可能取值为$ X=\left\{ x_{1},x_{2} ,.....,x_{n}   \right\}  $，其概率分布为$ P\left( X=x_{i}  \right) =p_{i} ,i=1,2,.....,n $，则随机变量$ X $的熵定义为$ H(X) $：

$ H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i}  \right) logP\left( x_{i}  \right) } =\sum_{i=1}^{n}{P\left( x_{i}  \right) \frac{1}{logP\left( x_{i}  \right) } }  $
\subsection{联合熵}

两个随机变量$ X $和$ Y $的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量$ XY $的不确定性的度量，用$ H(X,Y) $表示：

$ H\left( X,Y \right) =-\sum_{i=1}^{n}{\sum_{j=1}^{n}{P\left( x_{i} ,y_{j}  \right)} logP\left( x_{i},y_{j}   \right)  }  $
\subsection{条件熵}

在随机变量$ X $发生的前提下，随机变量$ Y $发生新带来的熵，定义为$ Y $的条件熵，用$ H(Y|X) $表示：

$ H\left(Y|X \right) =-\sum_{x,y}^{}{P\left( x,y \right) logP\left( y|x \right) }  $
条件熵用来衡量在已知随机变量$ X $的条件下，随机变量$ Y $的不确定性。

实际上，熵、联合熵和条件熵之间存在以下关系：

$ H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)  $
推导过程如下：

其中：

第二行推到第三行的依据是边缘分布$ P(x) $等于联合分布$ P(x,y) $的和；
第三行推到第四行的依据是把公因子$ logP(x) $乘进去，然后把x,y写在一起；
第四行推到第五行的依据是：因为两个sigma都有P(x,y)，故提取公因子$ P(x,y) $放到外边，然后把里边的$ -（log P(x,y) - log P(x)） $写成$ - log (P(x,y) / P(x) )  $；
第五行推到第六行的依据是：$ P(x,y) = P(x) * P(y|x) $，故$ P(x,y) / P(x) = P(y|x) $。
\subsection{相对熵}

相对熵又称互熵、交叉熵、$ KL $散度、信息增益，是描述两个概率分布P和Q差异的一种方法，记为$ D(P||Q) $。在信息论中，$ D(P||Q) $表示当用概率分布$ Q $来拟合真实分布$ P $时，产生的信息损耗，其中$ P $表示真实分布，$ Q $表示$ P $的拟合分布。

对于一个离散随机变量的两个概率分布$ P $和$ Q $来说，它们的相对熵定义为：

$ D\left( P||Q \right) =\sum_{i=1}^{n}{P\left( x_{i}  \right) log\frac{P\left( x_{i}  \right) }{Q\left( x_{i}  \right) } } $ 
注意：$ D(P||Q) ≠ D(Q||P) $

\subsection{互信息}

两个随机变量$ X $，$ Y $的互信息定义为$ X $，$ Y $的联合分布和各自独立分布乘积的相对熵称为互信息，用$ I(X,Y) $表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。

$ I\left( X,Y \right) =\sum_{x\in X}^{}{\sum_{y\in Y}^{}{P\left( x,y \right) } log\frac{P\left( x,y \right) }{P\left( x \right) P\left( y \right) } }  $
互信息、熵和条件熵之间存在以下关系： $ H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right)  $

推导过程如下：

通过上面的计算过程发现有：H(Y|X) = H(Y) - I(X,Y)，又由前面条件熵的定义有：H(Y|X) = H(X,Y) - H(X)，于是有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。

4-6、最大熵模型

最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。

前面我们知道，若随机变量X的概率分布是$ P\left( x_{i}  \right) $ ，则其熵定义如下：

$ H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i}  \right) logP\left( x_{i}  \right) } =\sum_{i=1}^{n}{P\left( x_{i}  \right) \frac{1}{logP\left( x_{i}  \right) } } $ 
熵满足下列不等式：

$ 0\leq H\left( X \right) \leq log\left| X \right|  $
式中，|X|是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。也就是说，当X服从均匀分布时，熵最大。

直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。


\section{title}数值计算

\subsection{上溢和下溢}

在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。

一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。

另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\varpi 或-\varpi 时发生上溢。进一步的运算通常将这些无限值变为非数字。

必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为：

当式中的x_{i} 都是很小的负数时，e^{x_{i} } 就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的x_{i} 是很大的正数时，e^{x_{i} } 就会发生上溢导致结果是未定的。

\subsection{计算复杂性与NP问题}

1、算法复杂性
现实中大多数问题都是离散的数据集，为了反映统计规律，有时数据量很大，而且多数目标函数都不能简单地求得解析解。这就带来一个问题：算法的复杂性。

算法理论被认为是解决各类现实问题的方法论。衡量算法有两个重要的指标：时间复杂度和空间复杂度，这是对算法执行所需要的两类资源——时间和空间的估算。

一般，衡量问题是否可解的重要指标是：该问题能否在多项式时间内求解，还是只能在指数时间内求解？在各类算法理论中，通常使用多项式时间算法即可解决的问题看作是易解问题，需要指数时间算法解决的问题看作是难解问题。

指数时间算法的计算时间随着问题规模的增长而呈指数化上升，这类问题虽然有解，但并不适用于大规模问题。所以当前算法研究的一个重要任务就是将指数时间算法变换为多项式时间算法。

2、确定性和非确定性 

除了问题规模与运算时间的比较，衡量一个算法还需要考虑确定性和非确定性的概念。

这里先介绍一下“自动机”的概念。自动机实际上是指一种基于状态变化进行迭代的算法。在算法领域常把这类算法看作一个机器，比较知名的有图灵机、玻尔兹曼机、支持向量机等。

所谓确定性，是指针对各种自动机模型，根据当时的状态和输入，若自动机的状态转移是唯一确定的，则称确定性；若在某一时刻自动机有多个状态可供选择，并尝试执行每个可选择的状态，则称为非确定性。

换个说法就是：确定性是程序每次运行时产生下一步的结果是唯一的，因此返回的结果也是唯一的；非确定性是程序在每个运行时执行的路径是并行且随机的，所有路径都可能返回结果，也可能只有部分返回结果，也可能不返回结果，但是只要有一个路径返回结果，那么算法就结束。

在求解优化问题时，非确定性算法可能会陷入局部最优。

3、NP问题

有了时间上的衡量标准和状态转移的确定性与非确定性的概念，我们来定义一下问题的计算复杂度。

P类问题就是能够以多项式时间的确定性算法来对问题进行判定或求解，实现它的算法在每个运行状态都是唯一的，最终一定能够确定一个唯一的结果——最优的结果。

NP问题是指可以用多项式时间的非确定性算法来判定或求解，即这类问题求解的算法大多是非确定性的，但时间复杂度有可能是多项式级别的。

但是，NP问题还要一个子类称为NP完全问题，它是NP问题中最难的问题，其中任何一个问题至今都没有找到多项式时间的算法。

机器学习中多数算法都是针对NP问题（包括NP完全问题）的。

\subsection{数值计算}

上面已经分析了，大部分实际情况中，计算机其实都只能做一些近似的数值计算，而不可能找到一个完全精确的值，这其实有一门专门的学科来研究这个问题，这门学科就是——数值分析（有时也叫作“计算方法”）；运用数值分析解决问题的过程为：实际问题→数学模型→数值计算方法→程序设计→上机计算求出结果。

计算机在做这些数值计算的过程中，经常会涉及到的一个东西就是“迭代运算”，即通过不停的迭代计算，逐渐逼近真实值（当然是要在误差收敛的情况下）。

\section{最优化}

本节介绍机器学习中的一种重要理论——最优化方法。

6-1、最优化理论

无论做什么事，人们总希望以最小的代价取得最大的收益。在解决一些工程问题时，人们常会遇到多种因素交织在一起与决策目标相互影响的情况；这就促使人们创造一种新的数学理论来应对这一挑战，也因此，最早的优化方法——线性规划诞生了。 

6-2、最优化问题的数学描述

最优化的基本数学模型如下：

它有三个基本要素，即：

设计变量：x是一个实数域范围内的n维向量，被称为决策变量或问题的解；
目标函数：f(x)为目标函数；
约束条件：h_{i} \left( x \right) =0称为等式约束，g_{i} \left( x \right) \leq 0为不等式约束，i=0,1,2,......

6-3、凸集与凸集分离定理

1、凸集

实数域R上（或复数C上）的向量空间中，如果集合S中任两点的连线上的点都在S内，则称集合S为凸集，如下图所示：


数学定义为：

设集合D\subset R^{n} ，若对于任意两点x,y\in D，及实数\lambda \left( 0\leq \lambda \leq 1 \right) 都有：

\lambda x+\left( 1-\lambda  \right) y\in D
则称集合D为凸集。

2、超平面和半空间

实际上，二维空间的超平面就是一条线（可以使曲线），三维空间的超平面就是一个面（可以是曲面）。其数学表达式如下：

超平面：H=\left\{ x\in R^{n} |a_{1} +a_{2}+...+a_{n} =b  \right\} 

半空间：H^{+} =\left\{ x\in R^{n} |a_{1} +a_{2}+...+a_{n} \geq b  \right\} 

3、凸集分离定理

所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：

4、凸函数

凸函数就是一个定义域在某个向量空间的凸子集C上的实值函数。


数学定义为：

对于函数f(x)，如果其定义域C是凸的，且对于∀x,y∈C，0\leq \alpha \leq 1， 有： 

f\left( \theta x+\left( 1-\theta  \right) y \right) \leq \theta f\left( x \right) +\left( 1-\theta  \right) f\left( y \right) 

则f(x)是凸函数。
注：如果一个函数是凸函数，则其局部最优点就是它的全局最优点。这个性质在机器学习算法优化中有很重要的应用，因为机器学习模型最后就是在求某个函数的全局最优点，一旦证明该函数（机器学习里面叫“损失函数”）是凸函数，那相当于我们只用求它的局部最优点了。

6-4、梯度下降算法

1、引入 

前面讲数值计算的时候提到过，计算机在运用迭代法做数值计算（比如求解某个方程组的解）时，只要误差能够收敛，计算机最后经过一定次数的迭代后是可以给出一个跟真实解很接近的结果的。

这里进一步提出一个问题，如果我们得到的目标函数是非线性的情况下，按照哪个方向迭代求解误差的收敛速度会最快呢？

答案就是沿梯度方向。这就引入了我们的梯度下降法。

2、梯度下降法

在多元微分学中，梯度就是函数的导数方向。

梯度法是求解无约束多元函数极值最早的数值方法，很多机器学习的常用算法都是以它作为算法框架，进行改进而导出更为复杂的优化方法。

在求解目标函数f\left( x \right) 的最小值时，为求得目标函数的一个凸函数，在最优化方法中被表示为：

minf\left( x \right) 
根据导数的定义，函数f\left( x \right) 的导函数就是目标函数在x上的变化率。在多元的情况下，目标函数f\left( x,y,z \right) 在某点的梯度grad f\left( x,y,z \right) =\left( \frac{\partial f}{\partial x}  ,\frac{\partial f}{\partial y},\frac{\partial f}{\partial z}  \right) 是一个由各个分量的偏导数构成的向量，负梯度方向是f\left( x,y,z \right) 减小最快的方向。

如上图所示，当需要求f\left( x \right) 的最小值时（机器学习中的f\left( x \right) 一般就是损失函数，而我们的目标就是希望损失函数最小化），我们就可以先任意选取一个函数的初始点x_{0} （三维情况就是\left( x_{0} ,y_{0} ,z_{0}  \right) ），让其沿着途中红色箭头（负梯度方向）走，依次到x_{1} ，x_{2} ，...，x_{n} （迭代n次）这样可最快达到极小值点。

梯度下降法过程如下：

输入：目标函数f\left( x \right) ，梯度函数g\left( x \right) =grad f\left( x \right) ，计算精度\varepsilon 

输出：f\left( x \right) 的极小值点x^{*} 

1、任取取初始值x_{0} ，置k=0；
2、计算f\left( x_{k}  \right) ；
3、计算梯度g_{k} =grad f\left( x_{k}  \right) ，当\left| \left| g_{k} \right|  \right| <\varepsilon 时停止迭代，令x^{*} =x_{k} ；
4、否则令P_{k} =-g_{k} ，求\lambda _{k} 使f\left( x_{k+1} \right) =minf\left( x_{k} +\lambda _{k} P_{k}  \right) ；
5、置x_{k+1} =x_{k} +\lambda _{k} P_{k} ，计算f\left( x_{k+1}\right) ，当\left| \left| f\left( x_{k+1}\right) -f\left( x_{k}\right)  \right| \right| <\varepsilon 或\left| \left| x_{k+1} -x_{k}  \right|  \right| <\varepsilon 时，停止迭代，令x^{*} =x_{k+1}  ；
6、否则，置k=k+1，转3。

6-5、随机梯度下降算法

上面可以看到，在梯度下降法的迭代中，除了梯度值本身的影响外，还有每一次取的步长\lambda _{k} 也很关键：步长值取得越大，收敛速度就会越快，但是带来的可能后果就是容易越过函数的最优点，导致发散；步长取太小，算法的收敛速度又会明显降低。因此我们希望找到一种比较好的方法能够平衡步长。

随机梯度下降法并没有新的算法理论，仅仅是引进了随机样本抽取方式，并提供了一种动态步长取值策略。目的就是又要优化精度，又要满足收敛速度。

也就是说，上面的批量梯度下降法每次迭代时都会计算训练集中所有的数据，而随机梯度下降法每次迭代只是随机取了训练集中的一部分样本数据进行梯度计算，这样做最大的好处是可以避免有时候陷入局部极小值的情况（因为批量梯度下降法每次都使用全部数据，一旦到了某个局部极小值点可能就停止更新了；而随机梯度法由于每次都是随机取部分数据，所以就算局部极小值点，在下一步也还是可以跳出）

两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
6-6、牛顿法

1、牛顿法介绍

牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。
或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。
2、牛顿法的推导

将目标函数f\left( x \right)  在x_{k} 处进行二阶泰勒展开，可得：

f\left( x \right) =f\left( x_{k}  \right) +f^{'} \left( x_{k}  \right) \left( x-x_{k}  \right) +\frac{1}{2} f^{''}\left( x_{k}  \right)  \left( x-x_{k}  \right) ^{2} 
因为目标函数f\left( x \right) 有极值的必要条件是在极值点处一阶导数为0，即：f^{'} \left( x \right) =0

所以对上面的展开式两边同时求导（注意x才是变量，x_{k} 是常量\Rightarrow f^{'} \left( x_{k}  \right) ,f^{''} \left( x_{k}  \right) 都是常量），并令f^{'} \left( x \right) =0可得：

f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0
即：

x=x_{k} -\frac{f^{'} \left( x_{k}  \right) }{f^{''} \left( x_{k}  \right) } 
于是可以构造如下的迭代公式：

x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k}  \right) }{f^{''} \left( x_{k}  \right) } 
这样，我们就可以利用该迭代式依次产生的序列\left\{x_{1},x_{2},....,   x_{k}  \right\} 才逐渐逼近f\left( x \right) 的极小值点了。

牛顿法的迭代示意图如下：


上面讨论的是2维情况，高维情况的牛顿迭代公式是：


式中， ▽f是f\left( x \right) 的梯度，即：

H是Hessen矩阵，即：

3、牛顿法的过程

1、给定初值x_{0} 和精度阈值\varepsilon ，并令k=0；
2、计算x_{k} 和H_{k} ；
3、若\left| \left| g_{k}  \right|  \right| <\varepsilon 则停止迭代；否则确定搜索方向：d_{k} =-H_{k}^{-1} \cdot g_{k} ；
4、计算新的迭代点：x_{k+1} =x_{k} +d_{k} ；
5、令k=k+1，转至2。
6-7、阻尼牛顿法

1、引入

注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现f\left( x_{k+1}  \right) >f\left( x_{k}  \right) 的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。

为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是x_{k} ，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子\lambda _{k} ，即：

\lambda _{k} = minf\left( x_{k}  +\lambda d_{k}  \right) 
2、算法过程

1、给定初值x_{0} 和精度阈值\varepsilon ，并令k=0；
2、计算g_{k} （f\left( x \right) 在x_{k} 处的梯度值）和H_{k} ；
3、若\left| \left| g_{k}  \right|  \right| <\varepsilon 则停止迭代；否则确定搜索方向：d_{k} =-H_{k}^{-1} \cdot g_{k} ；
4、利用d_{k} =-H_{k}^{-1} \cdot g_{k} 得到步长\lambda _{k} ，并令x_{k+1} =x_{k} +\lambda _{k} d_{k} 
5、令k=k+1，转至2。
6-8、拟牛顿法

1、概述

由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。

2、拟牛顿法的推导

先将目标函数在x_{k+1} 处展开，得到：

f\left( x \right) =f\left( x_{k+1}  \right) +f^{'} \left( x_{k+1}  \right) \left( x-x_{k+1}  \right) +\frac{1}{2} f^{''}\left( x_{k+1}  \right)  \left( x-x_{k+1}  \right) ^{2} 
两边同时取梯度，得：

f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)
取上式中的x=x_{k} ，得：

f^{'}\left( x_{k}  \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)
即：

g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k}  \right) 
可得：

H_{k}^{-1} \cdot \left( g_{k+1} -g_{k}  \right) =x_{k+1} -x_{k}
上面这个式子称为“拟牛顿条件”，由它来对Hessen矩阵做约束。

	\chapter{基本概念汇总}
	\section{基本概念}
	\begin{itemize}
		\item \textbf{确定性现象}：在一定条件下，必然发生的现象。
		\item \textbf{统计规律性}：在大量重复试验或观察中所呈现出的固有的规律性。
		\item \textbf{随机试验}：具有以下三个特点的试验称为随机试验。
		\begin{itemize}
			\item 可以再相同的条件下重复地进行；
			\item 每次试验的可能结果不止一个，并且能事先明确试验的所有可能结果；
			\item 进行一次试验前，不能确定哪一个结果会出现。
		\end{itemize} 
	
	随机试验一般可以使用大写斜体字母表示，比如$ E $ 。
		\item \textbf{样本空间}：我们将随机试验$ E $ 的所有可能结果组成的集合称为$ E $ 的样本空间，记为$ S $  。样本空间中的元素，即$ E $ 的每个结果，称为\textbf{样本点}。
		\item 随机事件：简称事件，是指试验$ E$ 的样本空间$ S$ 的子集。再每次试验中，当且仅当这一子集的恶一个样本点出现时，称这一事件发生。
		\item 频率：在相同条件下，进行了$ n $ 次试验，在这$ n $ 次试验中，事件$ A $ 发生的次数$ n_A $称为事件$ A $ 发生的频数。比值$ \frac{n_A}{n} $称为事件A发生的频率，并记成$ f_n (A) $ 。
		\item 概率：设$ E $ 为随机试验，$ S $ 是它的样本空间，对于$ E $ 的每一事件$ A $ 赋予一个实数，记为$ P(A) $ ，称为事件$ A $的概率。概率是表示事件在一次试验中发生的可能性的大小的数。
		\item 等可能概型：具有以下两个特点的随机试验$ E $ 称为等可能概型。
		\begin{itemize}
			\item 随机试验的样本空间只包括两个元素；
			\item 试验中每个基本事件发生的可能性相同。
		\end{itemize}
		等可能概型是概率论发展初期研究的主要对象，所以也称为古典概型。
		\item 条件概率：设$ A $ ，$ B $ 是两个事件，且$ P(A)>0 $ ，称
		\begin{center}
			$ P(B|A)=\frac{P(AB)}{P(A)} $
		\end{center}
		为事件$ A $ 发生的条件下事件$ B $ 发生的条件概率。
		\item 随机变量：设随机试验的样本空间为$ S={e}$ ，$ X=X(e)$ 是定义在样本空间$ S $上的实值单值函数，对于任意实数$ x$ ，集合$ {e|X(e)\le x}$有确定的概率，则称$ X=X(e)$ 为随机变量。
		\item 离散型随机变量：可能取到的值是有限多个或者可列无限多个的随机变量。
		\item 离散型随机变量的分布律：设离散型随机变量$ X $ 所有可能取的值为$ x_k(k=1, 2, \dots)$ ,$ X $ 取各个可能值的概率，即事件$ {X=x_k} $ 的概率，为
		\begin{center}
			$ P\left\{X=x_k\right\}=p_k, k=1, 2, \dots $
		\end{center}
		则称该式为离散型随机变量$ X $ 的分布律。分布律也可以用表格的形式表示：
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				$ X $ & $ x_1 $ & $ x_2 $ & \dots & $ x_n $ & \dots \\
				\hline 
				$ p_k $ & $ p_1 $ & $ p_2 $ & \dots & $ p_n $ & \dots \\
				\hline
			\end{tabular}
		\end{center}
	\end{itemize}

	\section{常用数学公式}
		\begin{equation}
			C_a ^r =\binom{a}{r}=\frac{a!}{r!(a-r)!}=\frac{a(a-1)\dots (a-r+1)}{r!}
		\end{equation}
	\section{重要公式的证明}
	\chapter{随机变量及其分布}
	\section{离散型随机变量及其分布律}
	\begin{tabular}{|l|c|c|}
		\hline
		常用分布 & 分布律 & 分布函数 \\
		\hline
		(0-1) 分布 & $
		P\left\lbrace X=k\right\rbrace =p^k\left( 1-p\right) ^{1-k},\hspace{1em}k=0,1 \hspace{1em}\left( 0<p<1\right) 
		$ & \\
		\hline
	\end{tabular}

\begin{tabular}{l*{6}{c}r}
	Team              & P & W & D & L & F  & A & Pts \\
	\hline
	Manchester United & 6 & 4 & 0 & 2 & 10 & 5 & 12  \\
	Celtic            & 6 & 3 & 0 & 3 &  8 & 9 &  9  \\
	Benfica           & 6 & 2 & 1 & 3 &  7 & 8 &  7  \\
	FC Copenhagen     & 6 & 2 & 1 & 3 &  5 & 8 &  7  \\
\end{tabular}
 



	\section{连续型随机变量及其概率密度}
	\section{多维随机变量及其分布}
	\section{随机变量的数学特征}
	\chapter{大数定律及中心极限定律}
	\chapter{参数估计}
	统计推断的基本问题可以分为两类，一类是估计问题，另一类是假设检验问题。
	\section{点估计}
	\begin{newdef}
		设总体$X$的分布函数的形式已知，但它的一个或多个参数未知，借助于总体$X$的一个样本来估计总体未知参数的值的问题，称为参数的点估计问题。
	\end{newdef}
点估计问题的一般提法如下：设总体$X$的分布函数$F(x;\theta)$的形式已知。$\theta$为待估参数，$X_1$，$X_2$，$\dots$，$X_n$是$X$的一个样本。$x_1$，$x_2$，$\dots$，$x_n$为相应的一个样本值。点估计的问题就是要构建一个适当的统计量$\hat{\theta}(X_1,X_2,\dots,X_n)$，用它的观测值$\hat{\theta}(x_1,x_2,\dots,x_n)$作为未知参数$\theta$的近似值，我们称$\hat{\theta}(X_1,X_2,\dots,X_n)$为$\theta$的估计量，称$\hat{\theta}(x_1,x_2,\dots,x_n)$为$\theta$的估计值。

由于估计量是样本的函数，因此对于不同的样本值，$\theta$的估计值一般是不相同的。同时，对于同一参数，用不同的估计方法求出的估计量可能也不相同。可以通过无偏性、有效性和相合性来评价估计量。
\subsection{无偏性}
\begin{newdef}
	假设$X_1$，$X_2$，$\dots$，$X_n$是总体$X$的一个样本，$\theta\in\Theta$是包含在通体$X$的分布中的待估参数（）$\Theta$是$\theta$的取值范围）。若估计量$\hat{\theta}(X_1,X_2,\dots,X_n)$为$\theta$的数学期望$E(\hat{\theta})$存在，且对于任意$\theta\in\Theta$有，
	\begin{center}
		$E(\hat{\theta})=\theta$
	\end{center}
则称$\hat{\theta}$是$\theta$的\textbf{无偏估计量}。
\end{newdef}

在科学技术中将$E(\hat{\theta})-\theta$称为以$\hat{\theta}$作为$\theta$的估计的系统误差。无偏估计的实质意义就是无系统误差。

设总体$X$的均值为$\mu $，方差为$\sigma^2>0$均未知，不论总体$X$服从什么分布，样本均值$\overline{x}$都是总体均值$\mu $的无偏估计，样本方差$ S^2 =\frac{1}{n-1}\sum\limits_{i=1}^{n}(X_i-\overline{X})^2$是总体方差的无偏估计，但估计量$ \frac{1}{n}\sum\limits_{i=1}^{n}(X_i-\overline{X})^2$却不是$\sigma^2$的无偏估计。
\subsection{有效性}
方差ssh是随机变量取值与其数学期望的偏离程度的度量，所以无偏估计以方差小者为好。进而引入估计量有效性的概念。
\begin{newdef}
	设$\hat{\theta}_1(X_1,X_2,\dots,X_n)$与$\hat{\theta}_2(X_1,X_2,\dots,X_n)$都是$\theta$的无偏估计量，若对于任意$\theta\in\Theta$，有
	\begin{center}
		$D(\hat{\theta}_1) \le D(\hat{\theta}_2)$
	\end{center}
	则至少对于某一个$\theta\in\Theta$，上式中的不等号成立，则称$D(\hat{\theta}_1)$较$D(\hat{\theta}_2)$有效。
\end{newdef}
\subsection{相合性}
\begin{newdef}
	设$\hat{\theta}(X_1,X_2,\dots,X_n)$为参数$\theta$的估计量，若对于任意$\theta\in\Theta$，当$n\to\infty$时，$\hat{\theta}(X_1,X_2,\dots,X_n)$以概率收敛于$\theta$则称有
	$\hat{\theta}$为$\theta$的相合估计量。
\end{newdef}
相合性意味着随着样本量的增大，一个估计量的值是否可以稳定于待估参数的真值。相合性时对一个估计量的基本要求，若估计量不具有相合性，那么无论将样本量取得多大，都不能将参数$\theta$估计的足够准确。
	
	构建估计量的方法通常有两种：\textbf{矩估计法}和\textbf{最大似然估计法}。
	
	\subsection{矩估计法}
	设$X$为概率密度为$f(x;\theta_1,\theta_2,\dots,\theta_n)$ 的连续型随机变量，或者$X$为分布律为$P\left\lbrace X=x\right\rbrace =p(x;\theta_1,\theta_2,\dots,\theta_n)$的离散型随机变量，其中的$\theta_1,\theta_2,\dots,\theta_n$为待估参数，$X_1,X_2,\dots,X_n$是来自$X$的样本。假设总体$X$的前$k$阶矩
	\begin{center}
		\begin{tabular}{r l}
			$ \mu_1 = E(X^l) = \int{-\infty}^{\infty} x^l f(x;\theta_1,\theta_2,\dots,\theta_n) dx$ & $X$为连续型\\
			$ \mu_1 = E(X^l) = \sum\limits_{x \in R_x} x^l p(x;\theta_1,\theta_2,\dots,\theta_n)$  & $X$为离散型\\
		\end{tabular}
	\end{center}
	($l=1,2,\dots,k$，\ $R_x$是$X$可能的取值范围)存在。一般来说，它们是$\theta_1,\theta_2,\dots,\theta_n$的函数，基于样本矩
	\begin{center}
		$ A_l= \frac{1}{n}\sum\limits_{i=1}^n X_i^l $
	\end{center}
依概率收敛于相应的总体矩阵$\mu_l(l=1,2,\dots,k)$,样本矩的连续函数依概率收敛于相应的总体矩的连续函数，我们就用样本矩作为相应的总体矩的估计量，以样本矩的连续函数作为相应的总体矩的连续函数的估计量，这种估计方法称为矩估计法。

矩估计法的具体做法如下：






	\chapter{假设检验}
	\section{假设检验问题的$ P $ 值法}
	\begin{exercise}
	设总体$ X \sim N(\mu ,\sigma ^{2})$ ，$ \mu $ 未知，$ \sigma ^{2}=100 $，现有样本$ x_1 $，$ x_2 $，\dots ，$ x_{52} $，算得$ \overline{x} = 62.75$ 。现在来检验假设：
	\begin{center}
		$ H_0:\mu \le \mu _0=60,\ H_1 :\mu >60 $。
	\end{center}
\end{exercise}

\begin{proof}
	采用$ Z $ 检验法，检验统计量为：
	\begin{equation*}
		Z=\frac{\overline{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
	\end{equation*}
	以数据代入，得$Z$的观察值为：
	\begin{equation*}
		Z=\frac{62.75 - 60}{\frac{10}{\sqrt{52}}}=1.983
	\end{equation*}
	概率
	\begin{equation*}
		P\left\lbrace Z \ge z_0\right\rbrace =P\left\lbrace Z\ge1.983\right\rbrace =1-\Phi(1.983)=0.0238
	\end{equation*}
	称之为$Z$检验法的右边检验的$p$值。
\end{proof}
\begin{newdef}
	假设检验问题的$ p $ 值（probability value）是由检验统计量的样本观测值得出的原假设可能被拒绝的最小显著性水平。
	在现在计算机统计软件中，一般都会给出检验问题的$ p $ 值。按照$ p $ 值的定义，对于任意指定的显著性水平$ \alpha $，就有：
	
	\begin{enumerate}
		\item 若$ p $ 值$ \le \alpha $，则在显著性水平$ \alpha $ 下拒绝 $ H_0 $ ；
		\item 若$ p $ 值$ > \alpha $，则在显著性水平$ \alpha $ 下接受 $ H_0 $ ；
	\end{enumerate}
这种利用$p$值来确定是否拒绝 $ H_0 $ 的方法，称为$p$值法。
\end{newdef}


	\chapter{方差分析}
	\chapter{回归分析}
	
	
\end{document}