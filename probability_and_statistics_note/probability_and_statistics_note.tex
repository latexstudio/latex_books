% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\documentclass[11pt]{book}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{ctex}

\usepackage{tikz}
\usepackage{xcolor}

\usepackage{environ}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage[english]{babel}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{xparse}
\usepackage[top=3cm,bottom=3cm,left=3cm,right=3cm,headsep=10pt,a4paper]{geometry}
\usepackage{pgfplots}
\usepackage[colorlinks, CJKbookmarks]{hyperref}
\usepackage{ulem}

\newtheorem{exercise}{\hspace{2em}\textbf{例}}[section]
\newtheorem{definition}{\hspace{2em}\textbf{定义}}


\usetikzlibrary{shapes,decorations}
\definecolor{bule}{RGB}{18,29,57}
\definecolor{bablue}{RGB}{248,248,248}
\definecolor{main}{RGB}{127,191,51}
\definecolor{seco}{RGB}{0,145,215}
\definecolor{thid}{RGB}{180,27,131}


\newcommand{\newfancytheoremstyle}[5]{%
  \tikzset{#1/.style={draw=#3, fill=#2,very thick,rectangle,
      rounded corners, inner sep=10pt, inner ysep=20pt}}
  \tikzset{#1title/.style={fill=#3, text=#2}}
  \expandafter\def\csname #1headstyle\endcsname{#4}
  \expandafter\def\csname #1bodystyle\endcsname{#5}
}

\newfancytheoremstyle{fancythrm}{blue!10}{seco}{\bfseries\sffamily}{\sffamily}

\makeatletter
\DeclareDocumentCommand{\newfancytheorem}{ O{\@empty} m m m O{fancythrm} }{%
  % define the counter for the theorem
  \ifx#1\@empty
    \newcounter{#2}
  \else
    \newcounter{#2}[#1]
    \numberwithin{#2}{#1}
  \fi
  %% define the "newthem" environment
  \NewEnviron{#2}[1][{}]{%
    \noindent\centering
    \begin{tikzpicture}
      \node[#5] (box){
        \begin{minipage}{0.93\columnwidth}
          \csname #5bodystyle\endcsname \BODY~##1
        \end{minipage}};
      \node[#5title, right=10pt] at (box.north west){
        {\csname #5headstyle\endcsname #3 \stepcounter{#2}\csname the#2\endcsname\; ##1}};
      \node[#5title, rounded corners] at (box.east) {#4};
    \end{tikzpicture}
  }[\par\vspace{.5\baselineskip}]
}
\makeatother


% Define new styles
% \newfancytheoremstyle{<name>}{inner color}{outer color}{head style}{body style}
\newfancytheoremstyle{fancydef}{green!10}{green}{\itshape\sffamily}{\sffamily}

% Define some new environments
% \newfancytheorem[<number within>]{<name>}{<head>}{<symbol>}[<style>]
\newfancytheorem[chapter]{newthem}{Theorem}{$\clubsuit$}
\newfancytheorem[section]{newcor}{Corollary 推论}{$\heartsuit$}
\newfancytheorem{newdef}{定义}{$\spadesuit$}[fancydef]

\newtheorem{math_example}{例}[section]

\usepackage{makeidx} % 务必放到最后

\newcommand{\importanttext}[1]{\uline{#1}}



\title{概率论与数理统计学习笔记}
\author{Roger Young}
\setCJKmainfont{楷体}

\date{编译时间：\today}

\begin{document}	
	\maketitle
	\tableofcontents
	\newpage
	
	\chapter{\LaTeX 基本使用说明}
	\section{入门常识}
	
	\LaTeX 会默认丢弃命令后的空白字符。如果希望在命令后的到一个空白字符，可以在命令后加上\{\}和一个空格，或者加上一个特殊的空白距离命令。
	\LaTeX 源文件中可以使用\%来表示注释，\LaTeX 在处理文件时，会忽略\%后的改行剩余文本。如果需要使用较长的注释（注释块），可以使用verbatim包。
		\section{页面布局}
				\subsection{分页、分段、断行和断字}
		有必要时\LaTeX 会进行必要的分页、断行和断字。在特殊情况下，需要使用命令指示\LaTeX 进行分页、分段、断行和断字。这些命令包括：
		\begin{itemize}
			\item $ \backslash \backslash $ ：连着的两个反斜线指示\LaTeX 另起一行，但不另起一段。
			\item $ \backslash newline $ 命令：等同于$ \backslash \backslash $；
			\item $ \backslash \backslash* $ 命令：强行断行后，还禁止分页；
			\item $ \backslash newpage $ 命令：另起一新页。
			\item $ \backslash hyphenation\{word list\} $ ：可以用来断字；
			\item $ \backslash- $：提示断字位置。
			\item $ \backslash linebreak $：强制断行；
			\item $ \backslash nolinebreak $：强制不断行；
			\item $ \backslash linebreak[priority] $：建议断行，priority可选值为0、1、2、3、4；
			\item $ \backslash nolinebreak[priority] $：建议不断行，priority可选值为0、1、2、3、4；
			\item $ \backslash cleardoublepage $ ：开始新的偶数页；
			\item $ \backslash clearpage $：开始新页，并且导致当前浮动的表格、图片等都输出。；
			\item $ \backslash newpage $：开始新页；
			\item $ \backslash enlargethispage\left\lbrace size\right\rbrace  $：扩大当前页；
			\item $ \backslash pagebreak $：强制分页；
			\item $ \backslash nopagebreak $：强制不分页。
			
		\end{itemize}
		\subsection{页眉设置}
		\subsection{页脚设置}
		\begin{tabular}{|l|l|}
			\hline
			$ \backslash footnote[number]\left\lbrace text\right\rbrace $ & 在当前页尾插入编号的脚标 \\
			\hline
		\end{tabular}
		\section{字体设置}

		\section{参考文献引用}
		\section{\LaTeX 环境简介}
		\section{数学公式}
		\section{作图功能}
		\LaTeX 可以通过内置的picture环境来创建简单的图形。对于创建更复杂的图形可以尝试以下包：
		\begin{itemize}
			\item beamer
			\item pgf, Portable Graphics Format
			\item tikz, TikZ
		\end{itemize}
	
		一个picture环境可以通过“$ \backslash begin\left\{picture\right\}(x, y)$ ”和“$ \backslash end\left\{picture\right\}$ ”，或者“$ \backslash begin\left\{picture\right\}(x, y)(x_0, y_0)$ ”和“$ \backslash end\left\{picture\right\}$”来创建。其中$ x $、$ y $、$ x_0 $、$ y_0 $和$ \backslash unitlength $相关。$ (x, y) $ 指示\LaTeX 为picture预留的大小。可选的参数$ (x_0, y_0)$是为picture预留的方形的的左下角的坐标。
		可以通过“$ \backslash setlength$ ”命令来设置：
		\begin{center}
			$ \backslash setlength\left\lbrace \backslash unitlength \right\rbrace \left\lbrace 1.2cm \right\rbrace$
		\end{center}
	
		绘图命名的格式一般如下：
		\begin{center}
			$ \backslash put(x, y)\left\lbrace object\right\rbrace  $
		\end{center}
	或者
	\begin{center}
		$ \backslash multiput(x, y)(\Delta x, \Delta y)\left\lbrace n\right\rbrace\left\lbrace object\right\rbrace  $
	\end{center}
	贝塞尔曲线是个例外，其格式为：
		\begin{center}
		$ \backslash qbezier(x_1, y_1)(x_2, y_2)(x_3, y_3) $
	\end{center}


常用的画图命令如下：\\
\begin{center}
	\begin{tabular}{|c|l|c|}
		\hline
		类型 & 命令 & 说明 \\
		\hline
		线段 & $ \backslash put(x, y)\left \lbrace \backslash line(x_1, y_1)\left\lbrace length\right\rbrace \right\rbrace $ & \\
		\hline
		箭头 & $ \backslash put(x, y)\left \lbrace \backslash vector(x_1, y_1)\left\lbrace length\right \rbrace \right \rbrace $ & \\		
		\hline
		圆 & $ \backslash put(x, y)\left \lbrace \backslash circle\left \lbrace diameter\right \rbrace \right \rbrace  $ & \\
		\hline
		文本 & $ \backslash put(x, y)\left \lbrace \$\ text\ \$ \right\rbrace  $ & \\ 
		\hline
		公式 & $ \backslash put(x, y)\left \lbrace \$\ formulas\ \$ \right\rbrace  $ & \\ 
		\hline
		椭圆形 & $ \backslash put(x, y)\left \lbrace \backslash oval(x, y)\right \rbrace $ & \\ 
		\hline
		椭圆形 & $ \backslash put(x, y)\left \lbrace \backslash oval(x, y)[position]\right \rbrace $ & \\ 
		\hline
		贝叶斯曲线 & $ \backslash qbezier(x_1, y_1)(x_2, y_2)(x_3, y_3) $ & \\ 
		\hline
	\end{tabular}
\end{center}
\begin{tikzpicture}
\begin{axis} [axis lines=center]
\addplot [domain=-3:3, thick, smooth] { x^3 - 5*x };
\end{axis}
\end{tikzpicture}
\chapter{机器学习的数学基础}
\section{概述}
我们知道，机器学习的特点就是：以计算机为工具和平台，以数据为研究对象，以学习方法为中心；是概率论、线性代数、数值计算、信息论、最优化理论和计算机科学等多个领域的交叉学科。所以本文就先介绍一下机器学习涉及到的一些最常用的的数学知识。
\section{线性代数}
\subsection{标量}
一个标量就是一个单独的数，一般用小写的的变量名称表示。
\subsection{向量}
一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排
列成一个方括号包围的纵柱：

我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。

\subsection{矩阵}
矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如\textbf{A}。 如果一个实数矩阵高度为$ m $，宽度为$ n $，那么我们说$ A \in R^{m \times n} $。
\begin{center}
	\begin{equation}
  A= \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n}\\ a_{21} & a_{22} & \cdots & a_{2n}\\ \cdots & \cdots & & \cdots\\ a_{m1} & a_{m2} & \cdots & a_{mn}\\ \end{bmatrix}
  \end{equation}
\end{center}
\subsection{张量}
几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格： 


其中表的横轴表示图片的宽度值，这里只截取0~319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）。

当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。

\subsection{范数}
有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。Lp 范数如下：
\begin{center}
	\begin{equation}
\left \| x \right \|_{p}\left ( \sum_{i}\left | x_{i} \right |^{p}  \right )^{\frac{1}{p}}
\end{equation}
\end{center}

所以：

L1范数$ \left| \left| x \right| \right| $：为x向量各个元素绝对值之和；

L2范数$ \left| \left| x \right|  \right| _{2} $ ：为x向量各个元素平方和的开方。

\subsection{特征分解}
许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量$ \nu $：

$ A\nu =\lambda \nu $

标量$ \lambda $被称为这个特征向量对应的特征值。 

使用特征分解去分析矩阵A时，得到特征向量构成的矩阵V和特征值构成的向量$ \lambda $，我们可以重新将A写作：

$ A=Vdiag\left( \lambda  \right) V^{-1} $

\subsection{奇异值分解（SVD）}
除了特征分解，还有一种分解矩阵的方法，被称为奇异值分解（SVD）。将矩阵分解为奇异向量和奇异值。通过奇异分解，我们会得到一些类似于特征分解的信息。然而，奇异分解有更广泛的应用。

每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。
奇异分解与特征分解类似，只不过这回我们将矩阵A分解成三个矩阵的乘积：

\begin{center}
	$ A=UDV^{T} $ 
\end{center}
假设A是一个$ m\times n $矩阵，那么U是一个$ m\times m $矩阵，D是一个$ m\times n $矩阵，V是一个$ n\times n $矩阵。

这些矩阵每一个都拥有特殊的结构，其中$ U $和$ V $都是正交矩阵，$ D $是对角矩阵（注意，$ D $不一定是方阵）。对角矩阵$ D $对角线上的元素被称为矩阵$ A $的奇异值。矩阵U的列向量被称为左奇异向量，矩阵V 的列向量被称右奇异向量。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。
\subsection{Moore-Penrose伪逆}

对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：

\begin{center}\begin{equation} Ax=y \end{equation}\end{center}
等式两边同时左乘左逆B后，得到：

\begin{center}\begin{equation}x=By\end{equation}\end{center}
是否存在唯一的映射将A映射到B取决于问题的形式。

如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。

Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为：

但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：

其中，矩阵$ U $，$ D $ 和$ V $ 是矩阵$ A $奇异值分解后得到的矩阵。对角矩阵$ D $ 的伪逆$ D^{+} $ 是其非零元素取倒之后再转置得到的。

\subsection{几种常用的距离}

设有两个n维变量$ A=\left[ x_{11}, x_{12},...,x_{1n}   \right] $和$ B=\left[ x_{21} ,x_{22} ,...,x_{2n}  \right] $ ，则下面可以定义一些常用的距离公式：

\paragraph{曼哈顿距离}

曼哈顿距离也称为城市街区距离，数学定义如下：

$ d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } $ 

曼哈顿距离的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sum(abs(vector1-vector2))
\paragraph{欧氏距离}

欧氏距离其实就是L2范数，数学定义如下： 

$ d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k}  \right) ^{2} } }  $
欧氏距离的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt((vector1-vector2)*(vector1-vector2).T)
\paragraph{闵可夫斯基距离}

从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：

$ d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k}  \right) ^{p} } }  $
实际上，当$ p=1 $时，就是曼哈顿距离；当$ p=2 $时，就是欧式距离。

\paragraph{切比雪夫距离}

切比雪夫距离就是$ L_{\varpi} $ ，即无穷范数，数学表达式如下：

$ d_{12} =max\left( \left| x_{1k}-x_{2k} \right|  \right)  $
切比雪夫距离额Python实现如下：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt(abs(vector1-vector2).max)
\paragraph{夹角余弦}

夹角余弦的取值范围为$ [-1,1] $，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。

机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：

$ cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } }  $
夹角余弦的Python实现：

from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))
\paragraph{汉明距离}

汉明距离定义的是两个字符串中不相同位数的数目。

例如：字符串$ ‘1111’ $与$ ‘1001’ $之间的汉明距离为2。

信息编码中一般应使得编码间的汉明距离尽可能的小。

汉明距离的Python实现：

from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print smstr
\paragraph{杰卡德相似系数}

两个集合$ A $和$ B $的交集元素在$ A $和$  $B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号$ J(A,B) $表示，数学表达式为：

$ J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| }  $
杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。

\paragraph{杰卡德距离}

与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：

$ J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| }  $
杰卡德距离的Python实现：

from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print dist.pdist(matV,'jaccard')
\section{概率}

\subsection{为什么使用概率}？ 

概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。

在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。

计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。

\subsection{随机变量}

随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，$ x_{1} $ 和$ x_{2} $  都是随机变量$ X $可能的取值。
对于向量值变量，我们会将随机变量写成$ X $，它的一个值为$ x $。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。

随机变量可以是离散的或者连续的。

\subsection{概率分布}

给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。
从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。
\subsection{条件概率}

很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。
我们将给定$ X=x $时，$ Y=y $发生的概率记为$ P\left( Y=y|X=x \right)  $，这个概率可以通过下面的公式来计算：

$ P\left( Y=y|X=x  \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }  $

\subsection{贝叶斯公式}

先看看什么是“先验概率”和“后验概率”，以一个例子来说明：

假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有：$ P(患病) = 0.1\% $；即：在没有做检验之前，我们预计的患病率为$ P(患病)=0.1\% $，这个就叫作"先验概率"。 

再假设现在有一种该病的检测方法，其检测的准确率为$ 95\% $；即：如果真的得了这种病，该检测法有95\%的概率会检测出阳性，但也有5\%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有95\%的概率检测出阴性，但也有5\%的概率检测为阳性。用概率条件概率表示即为：$ P(显示阳性|患病)=95\% $

现在我们想知道的是：在做完检测显示为阳性后，某人的患病率$ P(患病|显示阳性) $，这个其实就称为"后验概率"。

而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”，如下：

$ P\left( B|A \right) =\frac{P\left( A|B \right)P\left( B\right)  }{P\left( A \right) }  $
在这个例子里就是：

贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。

这里需要说明的是，上面的计算中除了利用了贝叶斯公式外，还利用了“全概率公式”，即：

$ P\left( A \right) =P\left( A|B \right)+P\left( A|\bar{B}  \right)   $

\subsection{期望}

在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。

假设$  $X是一个离散随机变量，其可能的取值有：$ \left\{ x_{1} ,x_{2} ,......,x_{n}  \right\} $ ，各个取值对应的概率取值为：$ P\left( x_{k} \right)  , k=1,2,......,n $，则其数学期望被定义为：

$ E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k}  \right) }  $
假设X是一个连续型随机变量，其概率密度函数为$ P\left( x \right) $ 则其数学期望被定义为：

$ E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx $

\subsection{方差 }

概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： 

$ Var\left( x \right) =E\left\{ \left[ x-E\left( x \right)  \right] ^{2}   \right\} =E\left( x^{2}  \right) -\left[ E\left( x \right)  \right] ^{2}  $

\subsection{协方差}

在概率论和统计学中，协方差被用于衡量两个随机变量X和Y之间的总体误差。数学定义式为：

$ Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right]  \right) \left( Y-E\left[ Y \right]  \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right] $ 

\subsection{常见分布函数}

\paragraph{0-1分布}

0-1分布是单个二值型离散随机变量的分布，其概率分布函数为：

$ P\left( X=1 \right) =pP\left( X=0 \right) =1-p $
\paragraph{几何分布}

几何分布是离散型概率分布，其定义为：在$ n $次伯努利试验中，试验$ k $次才得到第一次成功的机率。即：前$ k-1 $次皆失败，第$ k $次成功的概率。其概率分布函数为：

$ P\left( X=k \right) =\left( 1-p \right) ^{k-1} p $
性质：

$ E\left( X \right) =\frac{1}{p} Var\left( X \right) =\frac{1-p}{p^{2} }  $
\paragraph{二项分布}

二项分布即重复$ n $次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为$ p $，不发生的概率为$ 1-p $，则$ n $次重复独立试验中发生$ k $次的概率为：

$ P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}  $
性质：

$ E\left( X \right) =npVar\left( X \right) =np\left( 1-p \right)  $

\paragraph{高斯分布}

高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：


若随机变量$ X $服从一个数学期望为$ \mu $ ，方差为$ \sigma ^{2} $ 的正态分布，则我们将其记为：$ N\left( \mu ,\sigma^{2}  \right)  $。其期望值$ \mu $ 决定了正态分布的位置，其标准差$ \sigma $ （方差的开方）决定了正态分布的幅度。

\paragraph{指数分布}

指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为$ T $，已知元件使用了$ t $小时，它总共使用至少$ t+s $小时的条件概率，与从开始使用时算起它使用至少$ s $小时的概率相等。下面这些都属于指数分布：

婴儿出生的时间间隔
网站访问的时间间隔
奶粉销售的时间间隔 
指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间$ t $，就等同于$ t $之内没有任何婴儿出生，即：

$ P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t}  }{0!}=e^{-\lambda t} $ 
则：

$ P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}  $
如：接下来15分钟，会有婴儿出生的概率为：

$ P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53 $
指数分布的图像如下：


\paragraph{泊松分布}

日常生活中，大量事件是有固定频率的，比如：

某医院平均每小时出生3个婴儿
某网站平均每分钟有2次访问
某超市平均每小时销售4包奶粉 
它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。

泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为：

$ P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t}  }{n!}  $
其中：

$ P $表示概率，$ N $表示某种函数关系，$ t $表示时间，$ n $表示数量，$ 1 $小时内出生3个婴儿的概率，就表示为$  P(N(1) = 3)  $；$ λ $ 表示事件的频率。

还是以上面医院平均每小时出生3个婴儿为例，则$ \lambda =3 $；

那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：

$ P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025 $
同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：

$ P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0  \right) - P\left( N\left( 1 \right)=1  \right)\approx 0.8 $
【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！





	\chapter{基本概念汇总}
	\section{基本概念}
	\begin{itemize}
		\item \textbf{确定性现象}：在一定条件下，必然发生的现象。
		\item \textbf{统计规律性}：在大量重复试验或观察中所呈现出的固有的规律性。
		\item \textbf{随机试验}：具有以下三个特点的试验称为随机试验。
		\begin{itemize}
			\item 可以再相同的条件下重复地进行；
			\item 每次试验的可能结果不止一个，并且能事先明确试验的所有可能结果；
			\item 进行一次试验前，不能确定哪一个结果会出现。
		\end{itemize} 
	
	随机试验一般可以使用大写斜体字母表示，比如$ E $ 。
		\item \textbf{样本空间}：我们将随机试验$ E $ 的所有可能结果组成的集合称为$ E $ 的样本空间，记为$ S $  。样本空间中的元素，即$ E $ 的每个结果，称为\textbf{样本点}。
		\item 随机事件：简称事件，是指试验$ E$ 的样本空间$ S$ 的子集。再每次试验中，当且仅当这一子集的恶一个样本点出现时，称这一事件发生。
		\item 频率：在相同条件下，进行了$ n $ 次试验，在这$ n $ 次试验中，事件$ A $ 发生的次数$ n_A $称为事件$ A $ 发生的频数。比值$ \frac{n_A}{n} $称为事件A发生的频率，并记成$ f_n (A) $ 。
		\item 概率：设$ E $ 为随机试验，$ S $ 是它的样本空间，对于$ E $ 的每一事件$ A $ 赋予一个实数，记为$ P(A) $ ，称为事件$ A $的概率。概率是表示事件在一次试验中发生的可能性的大小的数。
		\item 等可能概型：具有以下两个特点的随机试验$ E $ 称为等可能概型。
		\begin{itemize}
			\item 随机试验的样本空间只包括两个元素；
			\item 试验中每个基本事件发生的可能性相同。
		\end{itemize}
		等可能概型是概率论发展初期研究的主要对象，所以也称为古典概型。
		\item 条件概率：设$ A $ ，$ B $ 是两个事件，且$ P(A)>0 $ ，称
		\begin{center}
			$ P(B|A)=\frac{P(AB)}{P(A)} $
		\end{center}
		为事件$ A $ 发生的条件下事件$ B $ 发生的条件概率。
		\item 随机变量：设随机试验的样本空间为$ S={e}$ ，$ X=X(e)$ 是定义在样本空间$ S $上的实值单值函数，对于任意实数$ x$ ，集合$ {e|X(e)\le x}$有确定的概率，则称$ X=X(e)$ 为随机变量。
		\item 离散型随机变量：可能取到的值是有限多个或者可列无限多个的随机变量。
		\item 离散型随机变量的分布律：设离散型随机变量$ X $ 所有可能取的值为$ x_k(k=1, 2, \dots)$ ,$ X $ 取各个可能值的概率，即事件$ {X=x_k} $ 的概率，为
		\begin{center}
			$ P\left\{X=x_k\right\}=p_k, k=1, 2, \dots $
		\end{center}
		则称该式为离散型随机变量$ X $ 的分布律。分布律也可以用表格的形式表示：
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				$ X $ & $ x_1 $ & $ x_2 $ & \dots & $ x_n $ & \dots \\
				\hline 
				$ p_k $ & $ p_1 $ & $ p_2 $ & \dots & $ p_n $ & \dots \\
				\hline
			\end{tabular}
		\end{center}
	\end{itemize}

	\section{常用数学公式}
		\begin{equation}
			C_a ^r =\binom{a}{r}=\frac{a!}{r!(a-r)!}=\frac{a(a-1)\dots (a-r+1)}{r!}
		\end{equation}
	\section{重要公式的证明}
	\chapter{随机变量及其分布}
	\section{离散型随机变量及其分布律}
	\begin{tabular}{|l|c|c|}
		\hline
		常用分布 & 分布律 & 分布函数 \\
		\hline
		(0-1) 分布 & $
		P\left\lbrace X=k\right\rbrace =p^k\left( 1-p\right) ^{1-k},\hspace{1em}k=0,1 \hspace{1em}\left( 0<p<1\right) 
		$ & \\
		\hline
	\end{tabular}

\begin{tabular}{l*{6}{c}r}
	Team              & P & W & D & L & F  & A & Pts \\
	\hline
	Manchester United & 6 & 4 & 0 & 2 & 10 & 5 & 12  \\
	Celtic            & 6 & 3 & 0 & 3 &  8 & 9 &  9  \\
	Benfica           & 6 & 2 & 1 & 3 &  7 & 8 &  7  \\
	FC Copenhagen     & 6 & 2 & 1 & 3 &  5 & 8 &  7  \\
\end{tabular}
 



	\section{连续型随机变量及其概率密度}
	\section{多维随机变量及其分布}
	\section{随机变量的数学特征}
	\chapter{大数定律及中心极限定律}
	\chapter{参数估计}
	统计推断的基本问题可以分为两类，一类是估计问题，另一类是假设检验问题。
	\section{点估计}
	\begin{newdef}
		设总体$X$的分布函数的形式已知，但它的一个或多个参数未知，借助于总体$X$的一个样本来估计总体未知参数的值的问题，称为参数的点估计问题。
	\end{newdef}
点估计问题的一般提法如下：设总体$X$的分布函数$F(x;\theta)$的形式已知。$\theta$为待估参数，$X_1$，$X_2$，$\dots$，$X_n$是$X$的一个样本。$x_1$，$x_2$，$\dots$，$x_n$为相应的一个样本值。点估计的问题就是要构建一个适当的统计量$\hat{\theta}(X_1,X_2,\dots,X_n)$，用它的观测值$\hat{\theta}(x_1,x_2,\dots,x_n)$作为未知参数$\theta$的近似值，我们称$\hat{\theta}(X_1,X_2,\dots,X_n)$为$\theta$的估计量，称$\hat{\theta}(x_1,x_2,\dots,x_n)$为$\theta$的估计值。

由于估计量是样本的函数，因此对于不同的样本值，$\theta$的估计值一般是不相同的。同时，对于同一参数，用不同的估计方法求出的估计量可能也不相同。可以通过无偏性、有效性和相合性来评价估计量。
\subsection{无偏性}
\begin{newdef}
	假设$X_1$，$X_2$，$\dots$，$X_n$是总体$X$的一个样本，$\theta\in\Theta$是包含在通体$X$的分布中的待估参数（）$\Theta$是$\theta$的取值范围）。若估计量$\hat{\theta}(X_1,X_2,\dots,X_n)$为$\theta$的数学期望$E(\hat{\theta})$存在，且对于任意$\theta\in\Theta$有，
	\begin{center}
		$E(\hat{\theta})=\theta$
	\end{center}
则称$\hat{\theta}$是$\theta$的\textbf{无偏估计量}。
\end{newdef}

在科学技术中将$E(\hat{\theta})-\theta$称为以$\hat{\theta}$作为$\theta$的估计的系统误差。无偏估计的实质意义就是无系统误差。

设总体$X$的均值为$\mu $，方差为$\sigma^2>0$均未知，不论总体$X$服从什么分布，样本均值$\overline{x}$都是总体均值$\mu $的无偏估计，样本方差$ S^2 =\frac{1}{n-1}\sum\limits_{i=1}^{n}(X_i-\overline{X})^2$是总体方差的无偏估计，但估计量$ \frac{1}{n}\sum\limits_{i=1}^{n}(X_i-\overline{X})^2$却不是$\sigma^2$的无偏估计。
\subsection{有效性}
方差ssh是随机变量取值与其数学期望的偏离程度的度量，所以无偏估计以方差小者为好。进而引入估计量有效性的概念。
\begin{newdef}
	设$\hat{\theta}_1(X_1,X_2,\dots,X_n)$与$\hat{\theta}_2(X_1,X_2,\dots,X_n)$都是$\theta$的无偏估计量，若对于任意$\theta\in\Theta$，有
	\begin{center}
		$D(\hat{\theta}_1) \le D(\hat{\theta}_2)$
	\end{center}
	则至少对于某一个$\theta\in\Theta$，上式中的不等号成立，则称$D(\hat{\theta}_1)$较$D(\hat{\theta}_2)$有效。
\end{newdef}
\subsection{相合性}
\begin{newdef}
	设$\hat{\theta}(X_1,X_2,\dots,X_n)$为参数$\theta$的估计量，若对于任意$\theta\in\Theta$，当$n\to\infty$时，$\hat{\theta}(X_1,X_2,\dots,X_n)$以概率收敛于$\theta$则称有
	$\hat{\theta}$为$\theta$的相合估计量。
\end{newdef}
相合性意味着随着样本量的增大，一个估计量的值是否可以稳定于待估参数的真值。相合性时对一个估计量的基本要求，若估计量不具有相合性，那么无论将样本量取得多大，都不能将参数$\theta$估计的足够准确。
	
	构建估计量的方法通常有两种：\textbf{矩估计法}和\textbf{最大似然估计法}。
	
	\subsection{矩估计法}
	设$X$为概率密度为$f(x;\theta_1,\theta_2,\dots,\theta_n)$ 的连续型随机变量，或者$X$为分布律为$P\left\lbrace X=x\right\rbrace =p(x;\theta_1,\theta_2,\dots,\theta_n)$的离散型随机变量，其中的$\theta_1,\theta_2,\dots,\theta_n$为待估参数，$X_1,X_2,\dots,X_n$是来自$X$的样本。假设总体$X$的前$k$阶矩
	\begin{center}
		\begin{tabular}{r l}
			$ \mu_1 = E(X^l) = \int{-\infty}^{\infty} x^l f(x;\theta_1,\theta_2,\dots,\theta_n) dx$ & $X$为连续型\\
			$ \mu_1 = E(X^l) = \sum\limits_{x \in R_x} x^l p(x;\theta_1,\theta_2,\dots,\theta_n)$  & $X$为离散型\\
		\end{tabular}
	\end{center}
	($l=1,2,\dots,k$，\ $R_x$是$X$可能的取值范围)存在。一般来说，它们是$\theta_1,\theta_2,\dots,\theta_n$的函数，基于样本矩
	\begin{center}
		$ A_l= \frac{1}{n}\sum\limits_{i=1}^n X_i^l $
	\end{center}
依概率收敛于相应的总体矩阵$\mu_l(l=1,2,\dots,k)$,样本矩的连续函数依概率收敛于相应的总体矩的连续函数，我们就用样本矩作为相应的总体矩的估计量，以样本矩的连续函数作为相应的总体矩的连续函数的估计量，这种估计方法称为矩估计法。

矩估计法的具体做法如下：






	\chapter{假设检验}
	\section{假设检验的基础知识}
	假设检验是统计推断的另一类重要问题。在\uline{总体的分布函数完全未知}或\uline{只知其形式、但不知其参数}的情况下，为了推断总体的某些未知特性，提出某些关于总体的假设。然后根据样本对所提出的假设做出是接受还是拒绝的决策。假设检验就是做出这一决策的过程。
	
	\begin{math_example}
		某车间用一台包装机器包装葡萄糖。袋装糖的净重是一个随机变量，它\importanttext{服从正态分布}。当机器正常工作时，其均值为$ 0.5kg $，标准差为$ 0.015kg $。某日开工后为检测机器是否正常工作，随机抽取它所包装的9袋糖，称得净重为（$ kg $）：
		\begin{center}
			0.497, 0.506, 0.518, 0.524, 0.498, 0.511, 0.520, 0.515, 0.5512
		\end{center}		
		问机器是否正常工作？
	\end{math_example}
	
	
	\section{假设检验问题的$ P $ 值法}
	\begin{exercise}
	设总体$ X \sim N(\mu ,\sigma ^{2})$ ，$ \mu $ 未知，$ \sigma ^{2}=100 $，现有样本$ x_1 $，$ x_2 $，\dots ，$ x_{52} $，算得$ \overline{x} = 62.75$ 。现在来检验假设：
	\begin{center}
		$ H_0:\mu \le \mu _0=60,\ H_1 :\mu >60 $。
	\end{center}
\end{exercise}

\begin{proof}
	采用$ Z $ 检验法，检验统计量为：
	\begin{equation*}
		Z=\frac{\overline{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
	\end{equation*}
	以数据代入，得$Z$的观察值为：
	\begin{equation*}
		Z=\frac{62.75 - 60}{\frac{10}{\sqrt{52}}}=1.983
	\end{equation*}
	概率
	\begin{equation*}
		P\left\lbrace Z \ge z_0\right\rbrace =P\left\lbrace Z\ge1.983\right\rbrace =1-\Phi(1.983)=0.0238
	\end{equation*}
	称之为$Z$检验法的右边检验的$p$值。
\end{proof}
\begin{newdef}
	假设检验问题的$ p $ 值（probability value）是由检验统计量的样本观测值得出的原假设可能被拒绝的最小显著性水平。
	在现在计算机统计软件中，一般都会给出检验问题的$ p $ 值。按照$ p $ 值的定义，对于任意指定的显著性水平$ \alpha $，就有：
	
	\begin{enumerate}
		\item 若$ p $ 值$ \le \alpha $，则在显著性水平$ \alpha $ 下拒绝 $ H_0 $ ；
		\item 若$ p $ 值$ > \alpha $，则在显著性水平$ \alpha $ 下接受 $ H_0 $ ；
	\end{enumerate}
这种利用$p$值来确定是否拒绝 $ H_0 $ 的方法，称为$p$值法。
\end{newdef}


	\chapter{方差分析}
	\chapter{回归分析}
	
	
\end{document}